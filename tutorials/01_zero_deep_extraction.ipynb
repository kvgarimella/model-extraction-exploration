{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PU0YndxHaGq"
   },
   "source": [
    "# High(-est) Fidelity Extraction of a Linear Model\n",
    "\n",
    "***Note: I'm running in this in Google Colab.***\n",
    "In this notebook, we will walk through a simple tutorial for stealing a linear model assuming that the attacker knows the model architecture and has query access to the victim model. This isn't really practical because most sensitive and deployed ML models are non-linear and deep,  but it's a good starting point to gain some intuition on how to cleverly extract the parameters of a model. This notebook is based off of the following paper: High Accuracy and High Fidelity Extraction of Neural Networks (Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, Nicolas Papernot @ USENIX 2020). \n",
    "\n",
    "- [paper](https://arxiv.org/abs/1909.01838)\n",
    "- [talk](https://youtu.be/PPU2a_A2ScI)\n",
    "\n",
    "In particular, we will extract a **functionally equivalent linear model**. Recall that a functionally equivalent extracted model behaves *precisely* as the victim model behaves across the entire input space. \n",
    "## 1. Imports\n",
    "Let's import some libraries. We will be using `jax` as our main library. You may find the following tutorials helpful. Much of the jax code in this tutorial came from the first reference below:\n",
    "\n",
    "- [Jax - Linear Regression](https://coax.readthedocs.io/en/latest/examples/linear_regression/jax.html)\n",
    "- [Jax - The Autodiff Cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVJ6TevzozmI"
   },
   "outputs": [],
   "source": [
    "import jax                                            # jax\n",
    "import jax.numpy as jnp                               # numpy from jax\n",
    "from sklearn.datasets import make_regression          # building a simple dataset\n",
    "from sklearn.model_selection import train_test_split  # splitting train and test\n",
    "\n",
    "from tqdm import tqdm                                 # progress bar\n",
    "import matplotlib.pyplot as plt                       # plotting library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZoW2Y8SHaG2"
   },
   "source": [
    "## 2. Creating the dataset\n",
    "Let's create a regression dataset of 10,000 samples. Let's also control the feature size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnTfcgTpHaG0"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qY_cqW6wHaGz",
    "outputId": "824f46a2-f844-4573-b555-7f4f745516d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 10) (250, 10) (750,) (250,)\n"
     ]
    }
   ],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=INPUT_DIM)    # create dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y)       # split dataset\n",
    "train_mean = jnp.mean(x_train, axis=0, keepdims=True)           # train mean\n",
    "train_std  = jnp.std(x_train, axis=0, keepdims=True)            # train std\n",
    "x_train = (x_train - train_mean) / train_std                    # norm. train\n",
    "x_test  = (x_test - train_mean) / train_std                     # norm. test\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAdJlcfGlBQC"
   },
   "source": [
    "So, we have now have a train and test dataset. The first axis corresponds to the batch while the second axis are the actual features. We've also normalized the dataset using standard practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQzrIy_THaGy"
   },
   "source": [
    "## 3. Creating a simple linear model\n",
    "\n",
    "We will create a $d$-dimensional linear model. That means our function maps $d$-dimensional vectors to a real value ($\\mathrm{R}^d \\rightarrow \\mathrm{R}$). `jax` **explicitly** handles randomness so let's generate a key and sample some weights for our linear model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6GO86W7uo3Sk",
    "outputId": "ed744be0-d633-45a4-c290-aeb72d20f978"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w': DeviceArray([-0.372111  ,  0.26423106, -0.18252774, -0.7368198 ,\n",
      "             -0.44030386, -0.15214427, -0.6713536 , -0.5908642 ,\n",
      "              0.73168874,  0.5673025 ], dtype=float32), 'b': DeviceArray([0.], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# model weights\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = {\n",
    "    'w': jax.random.normal(key, shape=(x_train.shape[1],)),  # weights\n",
    "    'b': jnp.array([0.])                                     # bias\n",
    "}\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Af6Z7pGYHaG7"
   },
   "source": [
    "## 4. Creating the forward pass, loss function, and update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIGuwADlpe2S"
   },
   "outputs": [],
   "source": [
    "def forward(params, X):\n",
    "  return jnp.dot(X, params['w']) + params['b']\n",
    "\n",
    "def loss_fn(params, X, y):\n",
    "  err = forward(params, X) - y\n",
    "  return jnp.mean(jnp.square(err))  \n",
    "\n",
    "# autodiff allows us to differeniate a function w.r.t its inputs (by default, the first argument)\n",
    "value_grad_fn = jax.value_and_grad(loss_fn) \n",
    "\n",
    "def update(params, grads, learning_rate):\n",
    "    return jax.tree_multimap(lambda p, g: p - learning_rate * g, params, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKE6MZSjHaG7"
   },
   "source": [
    "## 5. Training this model\n",
    "Now, we take the position of the victim. We have our collected dataset and a model which we'd like to train. Again, we aren't looking for a fancy model or high accuracy here; we are just simulating the process of some organization creating a (linear) model. It won't take long at all to train this simple linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jf9PPlUo0VFU",
    "outputId": "e804c5d0-8bc7-4b16-c345-f477f1bf7ec6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 42.91it/s]\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS=100\n",
    "learning_rate = 3e-1\n",
    "train_losses = []\n",
    "test_losses  = []\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    loss = loss_fn(params, x_test, y_test)\n",
    "    test_losses.append(loss)\n",
    "\n",
    "    loss, grads = value_grad_fn(params, x_train, y_train) \n",
    "    train_losses.append(loss)\n",
    "    params = update(params, grads, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AL0aiDeKHaG-"
   },
   "source": [
    "## 6. Plotting metrics\n",
    "Let's go ahead and plot our train and val loss curves. Again, these aren't that important for our task, but it's good to check that training proceded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "dkM6pJJq0VHz",
    "outputId": "62f055b6-74a1-40be-e308-bbc251da0273"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(5,5), sharex=True)\n",
    "ax[0].plot(train_losses)\n",
    "ax[0].set_title(\"Train Loss\")\n",
    "ax[0].grid()\n",
    "ax[1].plot(test_losses)\n",
    "ax[1].set_title(\"Val Loss\")\n",
    "ax[1].set_xlabel(\"Epoch\");\n",
    "ax[1].grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m5pKbg2-iXTG",
    "outputId": "ce627b38-6447-4e15-e7a0-72f89ad9ea4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': DeviceArray([-0.12844068], dtype=float32),\n",
       " 'w': DeviceArray([59.207184 , 21.589506 , 44.023746 , 71.02374  , 29.497866 ,\n",
       "              39.228783 , 26.272846 , 24.236612 , 60.160286 ,  4.2438154],            dtype=float32)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpyeA5vYHaG_"
   },
   "source": [
    "## 7. Functionally-Equivalent Model Extraction\n",
    "Now, let's assume that we are some malicious party that wants to steal the trained linear model. We have the following:\n",
    "\n",
    "- Access to the model architecture (but crucially, not the parameters!)\n",
    "- Query access (we can provide an input to the victim's model and observe the real-valued output)\n",
    "\n",
    "One obvious method would be to generate many input and output pairs and train a proxy model. **However, in this notebook, we will look at a more *surgical* approach to stealing a functionally-equivalent model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLA2JcK4KFvc"
   },
   "source": [
    "In particular, we know that our model assumes the following form:\n",
    "$$ f(x) = w^Tx + b$$\n",
    "where $w, x \\in \\mathrm{R}^d$ and $b \\in \\mathrm{R}^1$. First, we can immediately see that querying the model at $\\vec{x}=0$ will provide us with the bias (you can confirm this by looking at the `forward` function). Second, by passing in the [canonical basis vectors](https://en.wikipedia.org/wiki/Standard_basis), we will in return receive $w_i + b \\text{ } \\forall i \\in \\{1..d\\}$. Let's build these queries and steal the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnmiGYuTqunI"
   },
   "outputs": [],
   "source": [
    "QUERY_WEIGHT = jnp.eye(INPUT_DIM)       #  canonical basis vectors as a single batch\n",
    "QUERY_BIAS   = jnp.zeros((1,INPUT_DIM)) #  zero vector to retreive the bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6HFmmQzLaZ5"
   },
   "source": [
    "Let's go ahead and print these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KpnWTA81LZi-",
    "outputId": "9a3f5be4-5fd6-466b-ed84-cdd0c939d604"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(QUERY_WEIGHT); print(); print(QUERY_BIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZqxRAESHaHA",
    "outputId": "1e6fa39d-0f1e-42ec-a470-2b810f4ad99b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10) (1, 10)\n"
     ]
    }
   ],
   "source": [
    "print(QUERY_WEIGHT.shape, QUERY_BIAS.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0beM3j_wLmzn"
   },
   "source": [
    "## 8. Querying the victim model\n",
    "Now that we have prepared our queries, let's query the victim model and see if we can extract a functionally-equivalent model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiE2N9WR7Ew5"
   },
   "outputs": [],
   "source": [
    "out_w = forward(params, QUERY_WEIGHT)\n",
    "out_b = forward(params, QUERY_BIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GsKoy8llHaHB",
    "outputId": "83a3d307-82a0-4d98-fb3d-86387486e4bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,) (1,) (10,) (1,)\n"
     ]
    }
   ],
   "source": [
    "print(out_w.shape, out_b.shape, params['w'].shape, params['b'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRpF_r_MMF9q"
   },
   "source": [
    "## 9. Extracting the weights and bias of the linear model. \n",
    "\n",
    "Recall that our first query extracted $w_i + b$. Thus, to extract the exact weights, we just need to subtract the bias from each query that used the basis vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yr-AB7Nj7Noo",
    "outputId": "ebb0b075-7f83-4c01-8fca-2ff231896e95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(True, dtype=bool)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.all(jnp.isclose(params['w'], (out_w - out_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gO8lPqpTMdkw"
   },
   "source": [
    "Great, so now we have the weights of the linear model. The bias is trivial since we passed in the zero vector and received the bias directly from the victim model. Let's just confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "omyvJzdkHaHC",
    "outputId": "2bc46227-6ba5-4985-b8e1-fbe0a5c301b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(True, dtype=bool)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.all(jnp.isclose(params['b'], out_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8I2lFfFMpmA"
   },
   "source": [
    "Now, we have effectively stolen the linear model that the victim trained! As a malicious actor, we can now build our own functionally equivalent model. Let's ensure the models have the exact same output for some random datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TrAYLS9RHaHD",
    "outputId": "57e080ce-9bf2-42e2-fc77-b64f95027519"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56.26381]\n",
      "[56.26381]\n"
     ]
    }
   ],
   "source": [
    "functionally_equivalent_params = {\n",
    "    'w': out_w - out_b,  # stolen weights\n",
    "    'b': out_b           # stolen bias\n",
    "}\n",
    "\n",
    "key = jax.random.PRNGKey(10) # notice that without updating the random state, jax.random returns the same values for arrays of the same shape\n",
    "print(forward(params, jax.random.normal(key, shape=(INPUT_DIM,))))\n",
    "print(forward(functionally_equivalent_params, jax.random.normal(key, shape=(INPUT_DIM,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Z4r2cMhMyV6"
   },
   "source": [
    "So how many queries did it take us to steal the victim model? We batched up our canonical vectors and had one additional query to obtain the bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cFcx7FHwjgGZ",
    "outputId": "2e27b08e-a880-4276-b19b-285e707355c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functionally Equivalent Linear Model generated with 11 queries.\n"
     ]
    }
   ],
   "source": [
    "print(\"Functionally Equivalent Linear Model generated with {} queries.\".format(QUERY_WEIGHT.shape[0] + QUERY_BIAS.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMIbkKB8M8EG"
   },
   "source": [
    "Thus, we simply needed $d+1$ queries to the model in order to produce a functionally equivalent stolen model. While this is a rather simple example, it illustrates that having some information about the model architecture allows one to produce precise queries that can outmatch any standard method (e.g. machine-learning based) when trying to steal a victim model."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "01_zero_deep_extraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
